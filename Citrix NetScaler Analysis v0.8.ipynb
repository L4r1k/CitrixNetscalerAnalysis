{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Please read: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Somewhat automated notebook for Citrix NetScaler Analysis. Intended only as a starting point to help automate some of the analysis.\n",
    "\n",
    "- Fill in the 'Configuration' section below then run the notebook\n",
    "- Run the notebook \"Kernel -> Restart & Run All\"  \n",
    "- You should see a 'Results.xlsx' generated in the directory you specified as your 'ANALYSISPATH'. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import logging\n",
    "import gzip\n",
    "import shutil\n",
    "import re\n",
    "import base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', -1)\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.set_option('max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    level = logging.DEBUG,\n",
    "    format = '%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers = [logging.StreamHandler()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill this in carefully and then you should be able to run Kernel -> Restart and Run all and get an excel output with an overview of the relevant artifacts. Feel free to dig into the notebook itself and do some of your own searches. This notebook is meant only as a starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use forward slashes for the paths\n",
    "\n",
    "# Path where you've saved the netscaler logs (e.g. Workspace/Hostname/log)\n",
    "# Usually found in /var/log\n",
    "LOGPATH = Path('')\n",
    "\n",
    "# LEAVE BLANK IF YOU HAVE NO TEMPLATES\n",
    "# Path to where you've saved the recovered netscaler template XMLs (e.g. Workspace/Hostname/templates). \n",
    "# Usually found in:\n",
    "# /netscaler/portal/templates\n",
    "# /var/tmp/netscaler/portal/templates\n",
    "TEMPLATEPATH = Path('')\n",
    "\n",
    "# Path to analysis workspace \n",
    "# Logs will be extracted here - recommend nesting under dir for hostname and running notebook once per host)\n",
    "# e.g. ( 'Workspace/Hostname' )\n",
    "ANALYSISPATH = Path('')\n",
    "\n",
    "# IOCs/common suspicious terms to search all logs (excluding sh.log) for - \n",
    "# some such as 'uname' throw a lot of false positives so be careful. \n",
    "# Main thing to look for is entries by 'nobody' user\n",
    "searchFor = ['nobody'] #'find /netscaler/portal/scripts', 'exec', 'print', 'echo', 'crontab', '/portal']\n",
    "\n",
    "# IOCs/common suspicious terms to search sh.logs for\n",
    "shSearchFor = ['nobody', 'find', 'crontab', 'mv', 'LDR'] \n",
    "\n",
    "# HTTPAccess column names\n",
    "CNAMES = [\n",
    "    'client_ip_address',\n",
    "    'client_identity',\n",
    "    'user_id',\n",
    "    'time_received',\n",
    "    'timezone',\n",
    "    'request',\n",
    "    'status_code',\n",
    "    'size',\n",
    "    'referer (sic)',\n",
    "    'user_agent'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unzip logs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure analysis path exists\n",
    "Path.mkdir(ANALYSISPATH, parents=True, exist_ok=True)\n",
    "\n",
    "log_type_dict = {\n",
    "    'httpaccess.log.*.gz': 'httpaccess.log',\n",
    "    'cron.*.gz': 'cron',\n",
    "    'bash.*.gz': 'bash.log',\n",
    "    'notice.*.gz': 'notice.log',\n",
    "    'sh.*.gz': 'sh.log'\n",
    "}\n",
    "\n",
    "for zipped_wildcard, unzipped_name in log_type_dict.items():\n",
    "    for fp in LOGPATH.glob(zipped_wildcard):\n",
    "        logging.info('Unzipping {}'.format(fp))\n",
    "        with gzip.open(fp, 'rb') as f_in:\n",
    "            with open(f'{ANALYSISPATH}\\{fp.name[:-3]}', 'wb') as f_out:\n",
    "                logging.info(f'Writing to {LOGPATH}\\{fp.name[:-3]}')\n",
    "                shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "    with open(f'{LOGPATH}\\{unzipped_name}', 'rb') as f_in:\n",
    "        with open(f'{ANALYSISPATH}\\{unzipped_name}', 'wb') as f_out:\n",
    "            logging.info(f'Writing to {LOGPATH}\\{unzipped_name}')\n",
    "            shutil.copyfileobj(f_in, f_out)\n",
    "logging.info(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HTTPAccess Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fp in ANALYSISPATH.glob('httpaccess.*'):\n",
    "    logging.info(f'Pre-processing {fp}')\n",
    "    with open(fp, 'r', encoding='latin1') as f:\n",
    "        lines = f.readlines()\n",
    "    with open(f'{ANALYSISPATH}\\processed-{fp.name}', 'w', encoding='latin1') as f:\n",
    "        for line in lines:\n",
    "            if 'logfile turned over due to size' not in line:\n",
    "                f.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "for fp in ANALYSISPATH.glob('processed-httpaccess.*'):\n",
    "    logging.info('Opening {}'.format(fp))\n",
    "    partial_df = pd.read_csv(fp,\n",
    "                             names=CNAMES,\n",
    "                             delim_whitespace=True, \n",
    "                             na_values='-',\n",
    "                            )\n",
    "    df = df.append(partial_df, sort=True)\n",
    "    df = df.fillna('-')\n",
    "    df.time_received = df.time_received.str.strip('[')\n",
    "    df.timezone = df.timezone.str.strip(']')\n",
    "    df['timestamp'] = pd.to_datetime(df.time_received, format='%d/%b/%Y:%X')\n",
    "logging.info('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vpnRequests = df[(df['request'].str.contains('/vpn/\\.\\./vpns'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmls = vpnRequests[vpnRequests.request.str.contains('.xml')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse Payload XMLs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if TEMPLATEPATH != Path(''):\n",
    "    import xml.etree.ElementTree as ET\n",
    "    import hashlib\n",
    "    payloadDict = {}\n",
    "    for fp in TEMPLATEPATH.glob('*.xml'):\n",
    "        logging.info(\"Parsing {}\".format(fp))\n",
    "        with open(fp, 'r') as fin:\n",
    "            data = fin.read()\n",
    "            hex = hashlib.md5(data.encode()).hexdigest()\n",
    "        try:\n",
    "            root = ET.parse(fp).getroot()\n",
    "            username = None\n",
    "            desc = None\n",
    "            title = None\n",
    "            url = None\n",
    "            UI_inuse = None\n",
    "            username = root.get('username')\n",
    "            for type_tag in root.findall('bookmarks/bookmark'):\n",
    "                desc = type_tag.get('descr')\n",
    "                title = type_tag.get('title')\n",
    "                url = type_tag.get('url')\n",
    "                UI_inuse = type_tag.get('UI_inuse')\n",
    "            payloadDict[fp.name] = (hex, \n",
    "                                    pd.to_datetime(fp.stat().st_ctime, unit='s'), \n",
    "                                    pd.to_datetime(fp.stat().st_atime, unit='s'), \n",
    "                                    pd.to_datetime(fp.stat().st_mtime, unit='s'), \n",
    "                                    desc, \n",
    "                                    title, \n",
    "                                    url, \n",
    "                                    UI_inuse, \n",
    "                                    username)\n",
    "        except:\n",
    "            payloadDict[fp.name] = (hex, \n",
    "                                    pd.to_datetime(fp.stat().st_ctime, unit='s'), \n",
    "                                    pd.to_datetime(fp.stat().st_atime, unit='s'), \n",
    "                                    pd.to_datetime(fp.stat().st_mtime, unit='s'), \n",
    "                                    data, \n",
    "                                    None, \n",
    "                                    None, \n",
    "                                    None,\n",
    "                                    None)\n",
    "    payloadDf = pd.DataFrame(payloadDict).T.reset_index()\n",
    "    payloadDf.columns = ['File', 'MD5', 'Created_Time', 'AccessedTime', 'ModifiedTime', 'Desc', 'Title', 'Url', 'UI_inuse', 'username']\n",
    "    #payloadDf.Url = payloadDf.Url.str.replace('http', 'hxxp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decode Encoded Payloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEMPLATEPATH != Path(''):\n",
    "    b64pattern = re.compile(\"(?<=echo )[a-zA-Z0-9+\\/]+={0,2}\")\n",
    "    chrpattern = re.compile(\"(?<=readpipe\\()(.*)(?=\\)\\'\\})\")\n",
    "    def decodePayload(x):\n",
    "        matchedB64String = None\n",
    "        matchedChrString = None\n",
    "        if x['Desc']:\n",
    "            matchedB64String = b64pattern.search(x['Desc'])\n",
    "        if x['Title']:\n",
    "            matchedChrString = chrpattern.search(x['Title'])\n",
    "\n",
    "        if matchedB64String:\n",
    "            return base64.b64decode(matchedB64String.group(0)).decode()\n",
    "        elif matchedChrString:\n",
    "            replacedString = matchedChrString.group(0)\n",
    "            replacedString = replacedString.replace(\"chr(\", \"\")\n",
    "            replacedString = replacedString.replace(\") . \", \" \")\n",
    "            replacedString = replacedString.replace(\")\", \"\")\n",
    "            numArr = replacedString.split(\" \")\n",
    "            paddedNums = []\n",
    "            for num in numArr:\n",
    "                if len(num) == 2:\n",
    "                    paddedNums.append(f\"0{num}\")\n",
    "                else:\n",
    "                    paddedNums.append(num)\n",
    "            cleanStr = \"\"\n",
    "            for num in paddedNums:\n",
    "                cleanStr += chr(int(num))\n",
    "            return cleanStr\n",
    "        elif x['Desc']:\n",
    "            if 'BLOCK' in x['Desc'] or 'save config' in x['Desc'] or 'root' in x['Desc']:\n",
    "                return x['Desc']\n",
    "        elif x['Title']:\n",
    "            if 'BLOCK' in x['Title']:\n",
    "                return x['Title']\n",
    "        return x['Title']\n",
    "\n",
    "    payloadDf['DecodedPayloads'] = payloadDf.apply(decodePayload, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Log Parser "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyparsing import Word, alphas, Suppress, Combine, nums, string, Regex, Optional\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "class Parser(object):\n",
    "    # log lines don't include the year, but if we don't provide one, datetime.strptime will assume 1900\n",
    "    ASSUMED_YEAR = '2020'\n",
    "\n",
    "    def __init__(self):\n",
    "        ints = Word(nums)\n",
    "        month = Word(string.ascii_uppercase, string.ascii_lowercase, exact=3)\n",
    "        day   = ints\n",
    "        hour  = Combine(ints + \":\" + ints + \":\" + ints)\n",
    "\n",
    "        timestamp = month + day + hour\n",
    "        timestamp.setParseAction(lambda t: datetime.strptime(Parser.ASSUMED_YEAR + ' ' + ' '.join(t), '%Y %b %d %H:%M:%S'))\n",
    "        \n",
    "        # notice\n",
    "        notice = Suppress(\"<\") + Word(alphas + nums + \"/-_.()\") + Suppress(\">\")\n",
    "        # hostname\n",
    "        hostname = Word(alphas + nums + \"_-.\")\n",
    "        # appname\n",
    "        appname = Word(alphas + \"/-_.()\")(\"appname\") + (Suppress(\"[\") + ints(\"pid\") + Suppress(\"]\")) | (Word(alphas + \"/-_.\")(\"appname\"))\n",
    "        appname.setName(\"appname\")\n",
    "        # message\n",
    "        message = Regex(\".*\")\n",
    "\n",
    "        self._pattern = timestamp(\"timestamp\") + notice(\"notice\") + hostname(\"hostname\") + Optional(appname) + Suppress(':') + message(\"message\")\n",
    "\n",
    "    def parse(self, line):\n",
    "        try:\n",
    "            parsed = self._pattern.parseString(line)\n",
    "\n",
    "            for key in 'appname pid'.split():\n",
    "                if key not in parsed:\n",
    "                    parsed[key] = ''\n",
    "            return parsed.asDict()\n",
    "        except Exception as e:\n",
    "            logging.info(f\"Ignoring line: {line}{e}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse Cron Logs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cronDf = pd.DataFrame\n",
    "for fp in ANALYSISPATH.glob('cron*'):\n",
    "    with open(fp, 'r', encoding='latin1') as f:\n",
    "        logging.info(f'Parsing {fp}')\n",
    "        for line in f:\n",
    "            if cronDf.empty:\n",
    "                cronDf = pd.DataFrame(Parser().parse(line))\n",
    "            else:\n",
    "                cronDf = pd.concat([cronDf, pd.DataFrame(Parser().parse(line))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nobodyCronActions = cronDf[cronDf.message.str.contains('|'.join(searchFor))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse Bash Logs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bashDf = pd.DataFrame\n",
    "for fp in ANALYSISPATH.glob('bash*'):\n",
    "    with open(fp, 'r', encoding='latin1') as f:\n",
    "        logging.info(f'Parsing {fp}')\n",
    "        for line in f:\n",
    "            if bashDf.empty:\n",
    "                bashDf = pd.DataFrame(Parser().parse(line))\n",
    "            else:\n",
    "                bashDf = pd.concat([bashDf, pd.DataFrame(Parser().parse(line))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nobodyBashActions = bashDf[bashDf.message.str.contains('|'.join(searchFor))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse Notice Logs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noticeDf = pd.DataFrame\n",
    "for fp in ANALYSISPATH.glob('notice*'):\n",
    "    with open(fp, 'r', encoding='latin1') as f:\n",
    "        logging.info(f'Parsing {fp}')\n",
    "        for line in f:\n",
    "            if noticeDf.empty:\n",
    "                noticeDf = pd.DataFrame(Parser().parse(line))\n",
    "            else:\n",
    "                noticeDf = pd.concat([noticeDf, pd.DataFrame(Parser().parse(line))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nobodyNoticeActions = noticeDf[noticeDf.message.str.contains('|'.join(searchFor))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse sh Logs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "shDf = pd.DataFrame\n",
    "for fp in ANALYSISPATH.glob('sh*'):\n",
    "    with open(fp, 'r', encoding='latin1') as f:\n",
    "        logging.info(f'Parsing {fp}')\n",
    "        for line in f:\n",
    "            if shDf.empty:\n",
    "                shDf = pd.DataFrame(Parser().parse(line))\n",
    "            else:\n",
    "                shDf = pd.concat([shDf, pd.DataFrame(Parser().parse(line))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nobodyShActions = shDf[shDf.message.str.contains('|'.join(shSearchFor))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create DataFrame containing first/last date in logs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeDf = pd.DataFrame(columns=['log_type', 'start_date', 'end_date', 'timezone'])\n",
    "log_type_dfs = {\n",
    "    'cron': cronDf,\n",
    "    'bash': bashDf,\n",
    "    'notice': noticeDf,\n",
    "    'sh': shDf\n",
    "}\n",
    "\n",
    "http_start_time_row = df.loc[df.timestamp == min(df.timestamp)].iloc[0]\n",
    "http_end_time_row = df.loc[df.timestamp == max(df.timestamp)].iloc[0]\n",
    "timeDf = timeDf.append({'log_type': 'httpaccess', \n",
    "               'start_date': http_start_time_row.timestamp, \n",
    "               'end_date': http_end_time_row.timestamp, \n",
    "               'timezone': http_start_time_row.timezone}, ignore_index=True)\n",
    "\n",
    "for k, v in log_type_dfs.items():\n",
    "    timeDf = timeDf.append({\n",
    "        'log_type': k,\n",
    "        'start_date': v.loc[v.timestamp == min(v.timestamp)].iloc[0].timestamp,\n",
    "        'end_date': v.loc[v.timestamp == max(v.timestamp)].iloc[0].timestamp,\n",
    "        'timezone': None\n",
    "    }, ignore_index=True)\n",
    "    \n",
    "timeDf = timeDf.fillna('-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write Findings to Excel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = pd.ExcelWriter(ANALYSISPATH / 'Results.xlsx', engine='openpyxl')\n",
    "timeDf.to_excel(writer, sheet_name='LogTimeRanges', index=False)\n",
    "vpnRequests.to_excel(writer, sheet_name='VulnRelatedHTTPRequests', index=False)\n",
    "xmls.to_excel(writer, sheet_name='ObservedHTTPPayloadRequests', index=False)\n",
    "nobodyCronActions.to_excel(writer, sheet_name='SuspiciousCron', index=False)\n",
    "nobodyBashActions.to_excel(writer, sheet_name='SuspiciousBash', index=False)\n",
    "nobodyNoticeActions.to_excel(writer, sheet_name='SuspiciousNotice', index=False)\n",
    "nobodyShActions.to_excel(writer, sheet_name='SuspiciousSh', index=False)\n",
    "if TEMPLATEPATH != Path(''):\n",
    "    payloadDf.to_excel(writer, sheet_name='ParsedXMLs', index=False)\n",
    "writer.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
